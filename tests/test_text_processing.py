import pytest
from utils.text_processing import text_processor


def test_normalize_text():
    """–¢–µ—Å—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
    # –¢–µ—Å—Ç —Å –æ–±—ã—á–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
    text = "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞? üòä"
    normalized = text_processor.normalize_text(text)
    assert "–ø—Ä–∏–≤–µ—Ç" in normalized
    assert "üòä" not in normalized  # emoji –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–¥–∞–ª–µ–Ω—ã
    
    # –¢–µ—Å—Ç —Å URL
    text_with_url = "–ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ https://example.com —ç—Ç–æ—Ç —Å–∞–π—Ç"
    normalized = text_processor.normalize_text(text_with_url)
    assert "https://example.com" not in normalized
    
    # –¢–µ—Å—Ç —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏
    text_with_mentions = "–ü—Ä–∏–≤–µ—Ç @username –∏ #hashtag"
    normalized = text_processor.normalize_text(text_with_mentions)
    assert "@username" not in normalized
    assert "#hashtag" not in normalized
    
    # –¢–µ—Å—Ç —Å –∫–æ—Ä–æ—Ç–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º
    short_text = "Hi"
    normalized = text_processor.normalize_text(short_text)
    assert normalized == ""  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω


def test_lemmatize_text():
    """–¢–µ—Å—Ç –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
    text = "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—ã –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É—é—Ç –ø—Ä–æ–≥—Ä–∞–º–º—ã"
    lemmatized = text_processor.lemmatize_text(text)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ª–æ–≤–∞ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ
    assert "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç" in lemmatized or "–ø—Ä–æ–≥—Ä–∞–º–º–∞" in lemmatized


def test_extract_keywords():
    """–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    texts = [
        "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ python",
        "python —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞",
        "–∫–æ–¥ –Ω–∞ python"
    ]
    keywords = text_processor.extract_keywords(texts, max_keywords=5)
    assert len(keywords) > 0
    assert "python" in [kw.lower() for kw in keywords]


def test_calculate_similarity():
    """–¢–µ—Å—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏"""
    text1 = "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ python"
    text2 = "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ python"
    similarity = text_processor.calculate_similarity(text1, text2)
    assert 0 <= similarity <= 1
    
    # –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Å—Ö–æ–∂–µ—Å—Ç—å
    same_similarity = text_processor.calculate_similarity(text1, text1)
    assert same_similarity > 0.8


def test_remove_duplicates():
    """–¢–µ—Å—Ç —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    texts = [
        "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ python",
        "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ python",
        "–∫–æ–¥ –Ω–∞ python",
        "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ python"  # –¥—É–±–ª–∏–∫–∞—Ç
    ]
    unique_texts = text_processor.remove_duplicates(texts, similarity_threshold=0.7)
    assert len(unique_texts) < len(texts)  # –î—É–±–ª–∏–∫–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–¥–∞–ª–µ–Ω—ã


def test_filter_messages():
    """–¢–µ—Å—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π"""
    messages = [
        "–ö–æ—Ä–æ—Ç–∫–æ–µ",  # —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ
        "–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–ª–æ–≤",
        "",  # –ø—É—Å—Ç–æ–µ
        "–ï—â–µ –æ–¥–Ω–æ —Ö–æ—Ä–æ—à–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"
    ]
    filtered = text_processor.filter_messages(messages)
    assert len(filtered) == 2  # –¢–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –æ—Å—Ç–∞—Ç—å—Å—è
